%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% Welcome to Overleaf --- just edit your LaTeX on the left,
% and we'll compile it for you on the right. If you open the
% 'Share' menu, you can invite other users to edit at the same
% time. See www.overleaf.com/learn for more info. Enjoy!
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\documentclass{beamer}
\usepackage{algorithm} 
\usepackage{mathtools}
\usepackage{algpseudocode}
\usepackage{amssymb}
\usepackage{cancel}
\usepackage{listings}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{multirow}
\usepackage{xcolor}
%Information to be included in the title page:
\title{Progress Report 2:\\A Study on Convergence Results of Stochastic Gradient Methods}
\author{B09902055 Weiping Li, B09902073 Chun-Neng Chu}

\date{2023.5.16}

\begin{document}

\frame{\titlepage}

\begin{frame}
\frametitle{Outline}
\begin{enumerate}
        \item [1.] Past Progress
            \begin{enumerate}
                 \item [a.] Difference between Algorithms
                 \item [b.] Constraint Tradeoff
            \end{enumerate}
        \item [2.] Goals
        \item [3.] Standard SGD Convergence Proof Format
        \item [4.] Insights from ADAGRAD-Norm (Xiaoyu Li et al.)
            \begin{enumerate}
                 \item [a.] Non-arithmetic Lemmas
                 \item [b.] Necessity of Changing Update Sequence - Lemma 3
                 \item [c.] Necessity of Smoothness Constant - Lemma 3 \& 8
                 \item [d.] Comparison with ADAGRAD-Norm (Rachel Ward et al.)
            \end{enumerate}
        \item [5.] Future Plans
        \item [6.] QA
        \item [7.] Appendix
        \item [8.] References
    \end{enumerate}
\end{frame}

\begin{frame}
\frametitle{Difference between Algorithms: Update Sequence}
\begin{enumerate}
    \item [a.]
    \begin{algorithm}[H]
    	\caption{ADAGRAD-Norm (Xiaoyu Li et al.)} 
    	\begin{algorithmic}[1]
                \State Input: Initialize $x_0\in \textit{R}^d,b_0>0,\eta>0$
    		\For {$t=1,2,\ldots$}
    			\State Generate $\xi_{t-1},G_{t-1}=G(x_{t-1},\xi_{t-1})$
                    \color{red}\State $x_t\leftarrow x_{t-1}-\frac{\eta}{b_{t-1}}G_{t-1}$
                    \State $b_t^2\leftarrow b_{t-1}^2+||G_{t-1}||^2$\color{black}
    		\EndFor
	      \end{algorithmic} 
        \end{algorithm}
    \item [b.] 
    \begin{algorithm}[H]
    	\caption{ADAGRAD-Norm (Rachel Ward et al.)} 
    	\begin{algorithmic}[1]
                    \State Input: Initialize $x_0\in \textit{R}^d,b_0>0,\eta>0$
        		\For {$t=1,2,\ldots$}
        			\State Generate $\xi_{t-1},G_{t-1}=G(x_{t-1},\xi_{t-1})$
                        \color{blue}\State $b_t^2\leftarrow b_{t-1}^2+||G_{t-1}||^2$
                        \State $x_t\leftarrow x_{t-1}-\frac{\eta}{b_{t}}G_{t-1}$\color{black}
                        
        		\EndFor
    	\end{algorithmic} 
        \end{algorithm}
\end{enumerate}
\end{frame}



\begin{frame}
\frametitle{Constraint Tradeoff}
The two algorithms, with their individual constraints, can be proven to have the same complexity for convergence with respect to iteration $T$: $O(\frac{1}{\sqrt{T}})$ However, their constraints differ.
\begin{tabular}{ |p{6cm}||p{2cm}|p{2cm}|  }
 \hline
 \multicolumn{3}{|c|}{Constraints} \\
 \hline
 Constraints$\downarrow$ Algorithm $\rightarrow$ & Xiaoyu Li et al &Rachel Ward et al.\\
 \hline
 M-smooth   & \checkmark    &\checkmark\\
 \hline
 $E\left[\left\|\nabla f\left(x_t\right)-G\left(x_t, \xi_t\right)\right\|^2\right]\leq\sigma^2$&   \checkmark  & \checkmark\\
 \hline
 $E_{\xi}[G(x, \xi)]=$ $\nabla f(x)$ & \checkmark & \checkmark\\
 \hline
 $f > -\infty$    & \checkmark & \checkmark\\
 \hline
 know smoothness constant M
&\color{red}\checkmark  &  \\
 \hline
 L-Lipschitz &   & \color{blue}\checkmark\\
 \hline
\end{tabular}

\end{frame}

\begin{frame}
\frametitle{Goals}
\begin{enumerate}
        \item [1.] Find the reason for the different update sequence between the two algorithms. 
        \item [2.] Understand why Xiaoyu Li et al. require prior knowledge of specific smoothness constant in the proof.
        \item [3.] Understand why Rachel Ward et al. require Lipschitz constraint in proof.
        \item [4.] Can we acquire the same convergence rate with Rachel Ward et al.'s algorithm, but using the constraints of Xiaoyu Li et al.? Vice versa?
\end{enumerate}
\end{frame}

\begin{frame}
\frametitle{Standard SGD Convergence Proof Format}
Intuition: We want to find a complexity bound for $||\nabla f(x_t)||^a$ in the form of $O(\frac{1}{T^\alpha})$. Hence, we use the following thought process.\par

$\textcircled{1} \xleftarrow{\text{Markov's}} \textcircled{2} \leftarrow \textcircled{3} \xleftarrow{\text{Trick}} \textcircled{4} \xleftarrow{\text{Trick}} \textcircled{5}$\par

\begin{enumerate}
        \item [$\textcircled{1}$] $P([min_{1 \leq t < T}||\nabla f(x_t)||^2 = O(\frac{1}{T^\alpha})) \geq 1 - \delta$
        \item [$\textcircled{2}$] $E[min_{1 \leq t < T}||\nabla f(x_t)||^a]^{\frac{2}{a}} = O(\frac{1}{T^{\alpha}})$
        
        \item [$\textcircled{3}$] $E[(\sum_{t=1}^{T}||\nabla f(x_t)||^2)^\frac{a}{2}] = O(\frac{1}{T^{\alpha - \frac{a}{2}}})$
        
        \item [$\textcircled{4}$] $ \sum_{t=1}^{T} \eta_t^* E\left[\left\|\nabla f\left(x_{t-1}\right)\right\|^2\right] \leq f\left(x_0\right)-f^*+\sum_{t=1}^{T} \frac{\eta_t^2 ||G(x_{t-1}, \xi_{t-1})||^2 M}{2}$

        \item [$\textcircled{5}$] $|f(x_t)-f(x_{t-1})-\langle\nabla f(x_{t-1}), x_t-x_{t-1}\rangle| \leq \frac{M}{2}\|x_t-x_{t-1}\|^2$
\end{enumerate} \par
Note: $\eta_t$ is the learning rate at the $t^{th}$ iteration; $\eta_t^*$ may be $\eta_t$ or the estimation of $\eta_t$; $f^*$ is the optimal target function value.
\end{frame}

\begin{frame}
\frametitle{Xiaoyu Li et al. - Non-arithmetic Segments: Lemma 3}
When surveying Xiaoyu Li et al.'s article, we noticed that only two segments in the proof require non-arithmetic lemmas (3, 8).\par
\vspace{5mm}
Lemma 3: Assume $f$ is M-smooth and $E[G(x_{t-1},\xi_{t-1})]=\nabla f(x_{t-1})$. 
Then, the iterates of SGD with stepsizes $\eta_t \in R^{d}$ satisfy the following inequality
$\begin{aligned} E\left[\sum_{t=1}^T\left\langle\nabla f\left(x_{t-1}\right), \eta_t \nabla f\left(x_{t-1}\right)\right\rangle\right] \leq & f\left(x_{t-1}\right)-f^* \\  +\ &\frac{M}{2} E\left[\sum_{t=1}^T\left\|\eta_t G\left(x_{t-1}, \xi_{t-1}\right)\right\|^2\right]\end{aligned}$
\end{frame}

\begin{frame}
\frametitle{Xiaoyu Li et al. - Non-arithmetic Segments: Lemma 8}
Lemma 8: Assume $f$ is M-smooth, $E[G(x_{t-1},\xi_{t-1})]=\nabla f(x_{t-1})$ and the stochastic gradient satisfies \par\vspace{3mm}$E\left[\exp \left(\|\nabla f(x)-g(x, \xi)\|^2 / \sigma^2\right)\right] \leq \exp (1), \forall x$. Then, \par\vspace{3mm}
$\begin{aligned} E\left[\sum_{t=1}^T \eta_t^2\left\|G\left(x_{t-1}, \xi_{t-1}\right)\right\|^2\right] \leq &\ K+\frac{4 \eta^2}{b_0^2}(1+\ln T) \sigma^2 \\ & +\frac{4 \eta}{b_0} E\left[\sum_{t=1}^T \eta_t\left\|\nabla f\left(x_{t-1}\right)\right\|^2\right]
\end{aligned}$, \par\vspace{3mm} where \\
$K=2 \eta^2 \ln \left(\sqrt{b_0^2+2 T \sigma^2}+\sqrt{2} E\left[\sqrt{\sum_{t=1}^T\left\|\nabla f\left(x_{t-1}\right)\right\|^2}\right]\right)-2\eta^2 ln(b_0)$
\end{frame}

\begin{frame}
\frametitle{Xiaoyu Li et al. - Changing Update Sequence for Lemma 3}
In the proof of Lemma 3, there is an intermediary step that requires the following:
\par\vspace{5mm}
$E_{\xi_{t-1}}\left[\left\langle\nabla f\left(x_{t-1}\right), \eta_t\left(\nabla f\left(x_{t-1}\right)-G\left(x_{t-1}, \xi_{t-1}\right)\right)\right\rangle\right]=\left\langle\nabla f\left(x_{t-1}\right), \eta_t \nabla f\left(x_{t-1}\right)-\eta_t E_t\left[G\left(x_{t-1}, \xi_{t-1}\right)\right]\right\rangle=0$
\par\vspace{5mm}
This equation requires that $\eta_t$ is independent to $\xi_{t-1}$. The two terms are independent due to the fact that at the $t^{th}$ iteration, $\eta_t$ is decided by $\xi_0$ to $\xi_{t-2}$. Hence, $\eta_t$ can be taken out of the expectation.
\end{frame}

\begin{frame}
\frametitle{Xiaoyu Li et al. - Smoothness Constant from Lemma 3 \& 8} 
In the the next three slides, we demonstrate why concrete knowledge on the value of smoothness constant $M$ is necessary.
$\begin{aligned} & E\left[\sum_{t=1}^T \eta_t^2\left\|G\left(x_{t-1}, \xi_{t-1}\right)\right\|^2\right]\\ =&E\left[\color{red}\sum_{t=1}^T \eta_{t+1}^2\left\|G\left(x_{t-1}, \xi_{t-1}\right)\right\|^2\color{black}+\color{blue}\sum_{t=1}^T\left\|G\left(x_{t-1}, \xi_{t-1}\right)\right\|^2\left(\eta_t^2-\eta_{t+1}^2\right)\color{black}\right] \\  \leq & \color{red}2 \eta^2 \ln \left(\sqrt{b_0^2+2 T \sigma^2}+\sqrt{2} E\left[\sqrt{\sum_{t=1}^T\left\|\nabla f\left(x_{t-1}\right)\right\|^2}\right]\right) - 2\eta^2 ln(b_0)\\ \color{black}+ &\color{blue}\frac{4 \eta^2}{b_0^2}(1+\ln T) \sigma^2+\frac{4 \eta}{b_0} E\left[\sum_{t=1}^T \eta_t\left\|\nabla f\left(x_{t-1}\right)\right\|^2\right]\end{aligned}$

\end{frame}

\begin{frame}
\frametitle{Xiaoyu Li et al. - Smoothness Constant from Lemma 3 \& 8} 
Omitting the majority of tricks used by Xiaoyu Li et al., we can claim that the term $\color{red}\sqrt{2} E\left[\sqrt{\sum_{t=1}^T\left\|\nabla f\left(x_{t-1}\right)\right\|^2}\right]$ can be dropped, and $\color{red}\sum_{t=1}^T \eta_{t+1}^2\left\|G\left(x_{t-1}, \xi_{t-1}\right)\right\|^2$ is bounded by $O(ln(\sqrt{T}))$.
Intuitively speaking, the term $\color{blue}\sum_{t=1}^T\left\|G\left(x_{t-1}, \xi_{t-1}\right)\right\|^2\left(\eta_t^2-\eta_{t+1}^2\right)$ is the penalty caused by "borrowing" the red term, which is from the next iteration, to bound the stochastic gradient norm squared. With some tricks, it can be bounded by $\color{blue}\frac{4 \eta^2}{b_0^2}(1+\ln T) \sigma^2+\frac{4 \eta}{b_0} E\left[\sum_{t=1}^T \eta_t\left\|\nabla f\left(x_{t-1}\right)\right\|^2\right]$.\\
When lemma 3 is applied in an attempt to bound the term $E\left[\sum_{t=1}^T \eta_t\left\|\nabla f\left(x_{t-1}\right)\right\|^2\right]$, scaling the same term on the RHS of lemma 8 by $\frac{M}{2}$, we find the inequality on the next page.
\end{frame}

\begin{frame}
\frametitle{Xiaoyu Li et al. - Smoothness Constant from Lemma 3 \& 8}

\par\vspace{3mm}
$\begin{aligned} & \left(1-\frac{2 \eta M}{\sqrt{b_0^2}}\right) E\left[\sum_{t=1}^T \eta_t\left\|\nabla f\left(x_{t-1}\right)\right\|^2\right] \leq f\left(x_0\right)-f^{\star} \\  +M &\left(\eta^2 \ln \left(\sqrt{b_0^2+2 T \sigma^2}+\sqrt{2} E\left[\sqrt{\sum_{t=1}^T \left\|\nabla f\left(x_{t-1}\right)\right\|^2}\right]\right)-\frac{\eta^2 ln(b_0^2)}{2}\right) \\ & +\frac{2 \eta M}{b_0^2}(1+\ln T) \sigma^2 .\end{aligned}$\par
Consider the case where $\left(1-\frac{2 \eta M}{\sqrt{b_0^2}}\right) \leq 0$, we can see that the inequality always holds, and thus we gain no information of the term $E\left[\sum_{t=1}^T \eta_t\left\|\nabla f\left(x_{t-1}\right)\right\|^2\right]$. Hence we need to know the constant $M$ to initialize $\eta$ and $b_0$. 
\end{frame}

\begin{frame}
\frametitle{What Happens when Xiaoyu Li et al.'s Constraints Applied on Rachel Ward et al.}
\begin{enumerate}
        \item [1.] Since for the $t^{th}$ iteration, Rachel Ward et al. updates the learning rate ($\eta_t$) before the weights, we cannot take $\eta_t$ out of the expectation in the intermediary step that requires $E_{\xi_{t-1}}\left[\left\langle\nabla f\left(x_{t-1}\right), \eta_t\left(\nabla f\left(x_{t-1}\right)-G\left(x_{t-1}, \xi_{t-1}\right)\right)\right\rangle\right]=\left\langle\nabla f\left(x_{t-1}\right), \eta_t \nabla f\left(x_{t-1}\right)-\eta_t E_t\left[G\left(x_{t-1}, \xi_{t-1}\right)\right]\right\rangle=0$
        \par\vspace{3mm}
        \item [2.] In Rachel Ward et al.'s article, $\eta_t^*$ is an estimation of $\eta_t$ instead of exactly $\eta_t$. Since theorem 4 in Xiaoyu Li et al. requires descending $\eta_t^*$ and it is hard to confirm whether the estimation is indeed descending, the proof cannot be generalized to Rachel Ward et al. trivially.
\end{enumerate}
\end{frame}

\begin{frame}
\frametitle{Future Plans}
\begin{enumerate}

        \color{gray}\item [1.] Find the reason for the different update sequence between the two algorithms.
        \item [2.] Understand why Xiaoyu Li et al. require prior knowledge of specific smoothness constant in the proof.
        \color{black}\item [3.] Understand why Rachel Ward et al. require Lipschitz constraint in proof.
        \item [4.] \color{gray}Can we acquire the same convergence rate with Rachel Ward et al.'s algorithm, but using the constraints of Xiaoyu Li et al.?\color{black}\ Vice versa?
\end{enumerate}
\end{frame}

\begin{frame}
\frametitle{QA}

\end{frame}

\begin{frame}
\frametitle{Appendix: Proof of Lemma 3}
From the definition of M-smooth, we have
$|f(y)-f(x)-\langle\nabla f(x), y-x\rangle| \leq \frac{M}{2}\|y-x\|^2$
Thus,
$$
\begin{aligned}
f\left(x_{t}\right) & \leq f\left(x_{t-1}\right)+\left\langle\nabla f\left(x_{t-1}\right), {x}_{t}-{x}_{t-1}\right\rangle+\frac{M}{2}\left\|{x}_{t}-{x}_{t-1}\right\|^2 \\
& =f\left({x}_{t-1}\right)+\left\langle\nabla f\left({x}_{t-1}\right), {\eta}_t\left(\nabla f\left({x}_{t-1}\right)-{G}\left({x}_{t-1}, \xi_{t-1}\right)\right)\right\rangle \\ &-\left\langle\nabla f\left({x}_{t-1}\right), {\eta}_t \nabla f\left({x}_{t-1}\right)\right\rangle+\frac{M}{2}\left\|{\eta}_t {G}\left({x}_{t-1}, \xi_{t-1}\right)\right\|^2 .
\end{aligned}
$$
Taking the conditional expectation with respect to $\xi_0, \cdots, \xi_{t-2}$, we have that
$
E_{t-1}\left[\left\langle\nabla f\left({x}_{t-1}\right), {\eta}_t\left(\nabla f\left({x}_{t-1}\right)-{G}\left({x}_{t-1}, \xi_{t-1}\right)\right)\right\rangle\right]\\ &=\left\langle\nabla f\left({x}_{t-1}\right), {\eta}_t \nabla f\left({x}_{t-1}\right)-{\eta}_t {E}_t\left[{G}\left({x}_{t-1}, \xi_{t-1}\right)\right]\right\rangle=0 .
$\\
\end{frame}

\begin{frame}
\frametitle{Appendix: Proof of Lemma 3}
Hence, from the law of total expectation, we have
$
{E}\left[\left\langle\nabla f\left({x}_{t-1}\right), {\eta}_t \nabla f\left({x}_{t-1}\right)\right\rangle\right] \leq {E}\left[f\left({x}_{t-1}\right)-f\left({x}_{t}\right)+\frac{M}{2}\left\|{\eta}_t {g}\left({x}_{t-1}, \xi_{t-1}\right)\right\|^2\right] .
$
Summing over $t=1$ to $T$ and lower bounding $f\left({x}_{T}\right)$ with $f^{\star}$, we have the stated bound.
\end{frame}

\begin{frame}
\frametitle{Appendix: Proof of Lemma 8}
Lemma 10 states: If $x>0, \eta>0$, then $\ln (\frac{1}{x}) \geq \eta\left(1-x^{\frac{1}{\eta}}\right)$.
$$
\begin{aligned}
& {E}\left[\sum_{t=1}^T \eta_{t+1}^2\left\|{G}\left({x}_{t-1}, \xi_t\right)\right\|^2\right]={E}\left[\sum_{t=1}^T \frac{\eta^2\left\|{G}\left({x}_{t-1}, \xi_{t-1}\right)\right\|^2}{\left(b_0^2 +\sum_{i=1}^t\left\|{g}\left({x}_{i-1}, \xi_{i-1}\right)\right\|^2\right)}\right] \\
& \leq 2 \eta^2 {E}\left[\ln \left(\sqrt{b_0^2 +\sum_{t=1}^T\left\|{g}\left({x}_{t-1}, \xi_{t-1}\right)\right\|^2}\right]\right] \\
& \\
& \leq 2 \eta^2 \ln \left(\sqrt{b_0^2 +2 T \sigma^2}+\sqrt{2} {E}\left[\sqrt{\sum_{t=1}^T\left\|\nabla f\left({x}_{t-1}\right)\right\|^2}\right]\right)
\end{aligned}
$$
\end{frame}

\begin{frame}
\frametitle{Appendix: Proof of Lemma 8}
Where in first inequality we used Lemma 10 and in the third one we used Jensen's inequality.
Putting things together, we have
$$
\begin{aligned}
& {E}\left[\sum_{t=1}^T \eta_t^2\left\|{G}\left({x}_{t-1}, \xi_{t-1}\right)\right\|^2\right]\\&={E}\left[\sum_{t=1}^T \eta_{t+1}^2\left\|{G}\left({x}_{t-1}, \xi_{t-1}\right)\right\|^2+\sum_{t=1}^T\left\|{G}\left({x}_{t-1}, \xi_{t-1}\right)\right\|^2\left(\eta_t^2-\eta_{t+1}^2\right)\right] \\
& \leq 2 \eta^2 \ln \left(\sqrt{b_0^2 +2 T \sigma^2}+\sqrt{2} {E}\left[\sqrt{\sum_{t=1}^T\left\|\nabla f\left({x}_{t-1}\right)\right\|^2}\right]\right)\\&+\frac{4 \eta^2}{b_0^2 }(1+\ln T) \sigma^2+\frac{4 \eta}{b_0^2 ^{\frac{1}{2}}} {E}\left[\sum_{t=1}^T \eta_t\left\|\nabla f\left({x}_{t-1}\right)\right\|^2\right]
\end{aligned}
$$
\end{frame}



\begin{frame}
\frametitle{Appendix}
\begin{enumerate} 
    \item [1.] L-Lipschitz gradient implies L-smooth:\\
    \begin{enumerate}
        \item [(a)] $f$ is L-smooth if $f$ is continuously differentiable and $\forall x, y \in dom\ f,\ \langle \nabla f(y) - \nabla f(x), y - x \rangle \leq  L||y - x||^2$
        \item [(b)] $\nabla f$ is L-Lipschitz iff $\forall x, y \in dom\ f,\ ||\nabla f(y)-\nabla f(x)|| \leq L||y-x||$
        \item [(c)] Cauchy–Schwarz inequality states that $\forall u, v \in$ an inner product space, $|\langle u, v \rangle| \leq ||u||||v||$.
    \end{enumerate}
    By (a), (b), (c) can write\\ $\langle \nabla f(y) - \nabla f(x), y - x \rangle \leq ||\nabla f(y) - \nabla f(x)||||y - x|| \leq L ||y - x||^2$
    
    

    \item [2.] Convergence rate - expectation form to probability form:\\
    Markov's inequality: 
    $P(X\geq a)\leq \frac{E[X]}{a}$ ,if $X$ is a non-negative random variable and $a > 0$\\\\
    $\rightarrow P(min_{0\leq t \leq N} ||\nabla f(x_t)||^2 \geq \frac{E[(min_{0\leq t \leq N} ||\nabla f(x_t)||^2]}{\delta}) \leq \delta$\\
    $\rightarrow P(min_{0\leq t \leq N} ||\nabla f(x_t)||^2 \leq \frac{E[(min_{0\leq t \leq N} ||\nabla f(x_t)||^2]}{\delta}) \geq 1-\delta$\\
    $\rightarrow H(\delta, N, D^*)=\frac{E[(min_{0\leq t \leq N} ||\nabla f(x_t)||^2]}{\delta}$
    
\end{enumerate}
\end{frame}
\begin{frame}
\frametitle{References}
\begin{enumerate}
    \item Quoc Tran-Dinh. Sublinear Convergence Rates of Extragradient-Type Methods: A
Survey on Classical and Recent Developments. 
    \item Xiaoyu Li, Francesco Orabona. On the Convergence of Stochastic Gradient Descent.
with Adaptive Stepsizes
    \item Rachel Ward, Xiaoxia Wu, Léon Bottou. AdaGrad stepsizes: Sharp convergence over nonconvex
landscapes.
    \item Yen-Huan Li. Optimization algorithms Lecture 3.
    \item Chih-Jen Lin. Optimization Methods for Deep Learning: Convergence of stochastic gradient methods.
\end{enumerate}
\end{frame}

\end{document}